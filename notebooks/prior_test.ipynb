{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This note book is to test whether we can use any prior to determine the causal direction for non linear additive noise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/vol/cuda/11.2.1-cudnn8.1.0.77/targets/x86_64-linux/lib:/vol/cuda/11.2.1-cudnn8.1.0.77/lib64:'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/vol/bitbucket/ad6013/Research/gp-causal\")\n",
    "from models.PartObsBayesianGPLVM import PartObsBayesianGPLVM\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "from gpflow.config import default_float\n",
    "import tensorflow_probability as tfp\n",
    "from data.get_data import get_synthetic_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause, effect, weight = get_synthetic_dataset(100, 100, \"add_a\", \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 18:55:17.529470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training everything\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 18:55:17.540534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:17.540667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:17.541284: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-05 18:55:17.542346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:17.542691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:17.543110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:18.002636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:18.002785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:18.002896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:55:18.002997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "2022-08-05 18:55:18.280590: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-08-05 18:55:19.916140: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x1c618f90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training everything\n",
      "Run: 0, -13.86380291864802, -59.73796523415062\n",
      "1\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 1, -23.554811436005465, -56.15076104416811\n",
      "2\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 2, -8.412081490910424, -78.28121443799773\n",
      "3\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 3, -26.01546654309992, -84.8068396447151\n",
      "4\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 4, 14.22998986470796, -51.49220681625175\n",
      "5\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 5, 4.434526297139314, -57.450954319992576\n",
      "6\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 6, -10.82223446313381, -78.10963352426904\n",
      "7\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 7, -31.687348019403444, -67.10627741256715\n",
      "8\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 8, -12.505043590722323, -80.64108623445605\n",
      "9\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 9, -25.85005157937678, -68.48607322711239\n",
      "10\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 10, -10.391727664322275, -65.57846727811629\n",
      "11\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 11, -29.282705674856473, -55.46959237302627\n",
      "12\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 12, -5.689161943958453, -73.3808807188708\n",
      "13\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 13, -26.389402100718385, -88.71863013284964\n",
      "14\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 14, 22.349001357920912, -53.868030056321274\n",
      "15\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 15, -12.66779372410224, -58.04815306730676\n",
      "16\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 16, -37.49198389486037, -70.51981477343404\n",
      "17\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 17, -10.009927969092757, -66.49731052621722\n",
      "18\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 18, -20.40577729856328, -57.79291464991333\n",
      "19\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 19, -31.508941513750244, -64.5016771048265\n",
      "20\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 20, -2.53336143536427, -65.08244289613653\n",
      "21\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 21, -24.586005845380654, -65.4380151107253\n",
      "22\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 22, -16.969829185273525, -64.81612119012144\n",
      "23\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 23, -37.66578489380095, -88.26627826805336\n",
      "24\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 24, -17.072250624293673, -77.86254862837106\n",
      "25\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 25, -23.321992108284803, -59.43146165624729\n",
      "26\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 26, -0.02240417717862897, -69.75614953495524\n",
      "27\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 27, -26.709810206577956, -67.27365635218173\n",
      "28\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 28, -31.027603408722086, -69.95706410028585\n",
      "29\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 29, -52.39967243982382, -68.33057679774431\n",
      "30\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 30, 14.001472552577013, -52.0108280832617\n",
      "31\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 31, 35.39426939141882, -56.9052242150574\n",
      "32\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 32, 2.3656086701418815, -32.3228689266987\n",
      "33\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 33, -27.88412358803825, -62.24476045615526\n",
      "34\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 34, 34.915984958619106, -43.85877637908324\n",
      "35\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 35, -12.193014810119166, -57.493838841477725\n",
      "36\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 36, 13.011480439642128, -61.1952346810947\n",
      "37\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 37, 27.156103814377218, -68.0184666312269\n",
      "38\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 38, -10.877269660723357, -59.02079617337415\n",
      "39\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 39, -26.525090141101344, -55.65626657138833\n",
      "40\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 40, -17.215595679156024, -56.79687486759916\n",
      "41\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 41, -19.227643944712995, -53.96061567619731\n",
      "42\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 42, -9.953579267951511, -58.112767199615604\n",
      "43\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 43, 15.961972594126394, -52.00927598401839\n",
      "44\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 44, 17.877093028116064, -55.86247716055236\n",
      "45\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 45, -2.5359222441940688, -50.32224862294157\n",
      "46\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 46, -22.957872707497813, -67.95451614900163\n",
      "47\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 47, -10.462026295292276, -65.34475664405274\n",
      "48\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 48, -48.167848687209386, -67.6685137881992\n",
      "49\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 49, -2.87369465223, -46.693803712545076\n",
      "50\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 50, -16.06176369235196, -75.0720882725768\n",
      "51\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 51, -11.053691080210939, -78.39279032300234\n",
      "52\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 52, -27.63491363022993, -67.382513088503\n",
      "53\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 53, -22.071353753528882, -49.75829943711611\n",
      "54\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 54, 8.10962506787078, -45.90623833995272\n",
      "55\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 55, -27.783087334676722, -72.77210645024465\n",
      "56\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 56, 8.421508443828031, -59.079080458905935\n",
      "57\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 57, -13.76868917413475, -47.37112187524153\n",
      "58\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 58, -26.353967708630222, -72.26024624886298\n",
      "59\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 59, 0.11261889659255075, -54.75207168838193\n",
      "60\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 60, -7.5205393467603585, -56.17808594718561\n",
      "61\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 61, -1.377062399207432, -57.24849294051525\n",
      "62\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 62, 1.6881198653193223, -52.38221089434114\n",
      "63\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 63, 1.7713509858306509, -54.246564613538936\n",
      "64\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 64, 8.47405091494619, -50.88547007906796\n",
      "65\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 65, 2.028792485970456, -60.79071938831362\n",
      "66\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 66, -2.5338053524517647, -47.22855692588135\n",
      "67\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 67, 13.906937799953454, -52.04712833959496\n",
      "68\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 68, -28.491003016822546, -82.74094596880971\n",
      "69\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 69, -53.186069124799516, -66.14452338631521\n",
      "70\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 70, -41.00512317769716, -89.93156781657117\n",
      "71\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 71, -36.449207779295094, -68.1698171485652\n",
      "72\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 72, 23.425338666369584, -54.430307788158984\n",
      "73\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 73, 13.213595891565774, -71.52812558813987\n",
      "74\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 74, 22.042771978266416, -59.55074287591644\n",
      "75\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 75, -6.812900712310835, -44.411336132184616\n",
      "76\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 76, -19.91120679970217, -58.690737830332694\n",
      "77\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 77, 19.29268790936075, -47.28241008792102\n",
      "78\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 78, -33.0159283758791, -79.65152573027333\n",
      "79\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 79, -25.636729868602075, -75.33235694434566\n",
      "80\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 80, -15.727981162180214, -61.88088189214203\n",
      "81\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 81, 9.868812455624578, -51.93586063632635\n",
      "82\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 82, -32.24853227170445, -71.31440992846466\n",
      "83\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 83, -4.663442565140343, -59.41122030414802\n",
      "84\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 84, -4.912516910125191, -46.43196203409417\n",
      "85\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 85, 4.427588321367978, -42.111490100349144\n",
      "86\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 86, -21.17117385117004, -55.97150775839067\n",
      "87\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 87, 1.7859761872081492, -51.17346135764764\n",
      "88\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 88, -12.526254166328783, -57.96036037795106\n",
      "89\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 89, -26.03113124389226, -70.38946389628815\n",
      "90\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 90, 37.01960812555285, -47.47079651296271\n",
      "91\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 91, -13.458391542749013, -57.997769302565274\n",
      "92\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 92, -23.75099215703476, -45.19564000402836\n",
      "93\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 93, -12.737146089216338, -82.51084023569311\n",
      "94\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 94, -15.574848046862058, -78.63755262504742\n",
      "95\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 95, -19.82881137430904, -68.84462202779498\n",
      "96\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 96, -36.869549648546425, -65.87473240634674\n",
      "97\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 97, -20.93394425209874, -51.87859149441972\n",
      "98\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 98, -5.672008618693411, -56.308526576878535\n",
      "99\n",
      "Training everything\n",
      "Training everything\n",
      "Run: 99, -15.584303073009679, -75.38357504634949\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from gpflow.base import Parameter\n",
    "from gpflow.utilities import positive\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from models.BayesGPLVM import BayesianGPLVM\n",
    "\n",
    "\n",
    "correct = 0\n",
    "for idx in range(len(cause)):\n",
    "    print(idx)\n",
    "    x, y = cause[idx], effect[idx]\n",
    "\n",
    "    x = StandardScaler().fit_transform(x).astype(np.float64)\n",
    "    y = StandardScaler().fit_transform(y).astype(np.float64) \n",
    "\n",
    "\n",
    "    # # Conditional for cause\n",
    "    # # Define kernel\n",
    "    # sq_exp = gpflow.kernels.SquaredExponential(lengthscales=[0.01])\n",
    "    # sq_exp.variance.assign(1)\n",
    "    # linear_kernel = gpflow.kernels.Linear(variance=1)\n",
    "    # kernel = gpflow.kernels.Sum([sq_exp, linear_kernel])\n",
    "    # # Initialise approx posteroir and prior\n",
    "    # X_mean_init = 0.1 * tf.cast(y, default_float())\n",
    "    # X_var_init = tf.cast(\n",
    "    #     np.random.uniform(0, 0.1, (y.shape[0], 1)), default_float()\n",
    "    # )\n",
    "    # x_prior_var = tf.ones((y.shape[0], 1), dtype=default_float())\n",
    "    # inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "    #     np.random.randn(500, 1),\n",
    "    # )\n",
    "\n",
    "    # # Define marginal model\n",
    "    # marginal_model = BayesianGPLVM(\n",
    "    #     data=x,\n",
    "    #     kernel=kernel,\n",
    "    #     X_data_mean=X_mean_init,\n",
    "    #     X_data_var=X_var_init,\n",
    "    #     X_prior_var=x_prior_var,\n",
    "    #     jitter=1e-4,\n",
    "    #     inducing_variable=inducing_variable\n",
    "    # )\n",
    "    # marginal_model.likelihood.variance = Parameter(\n",
    "    #     0.1, transform=positive(1e-6)\n",
    "    # )\n",
    "    # # Train everything\n",
    "    # tf.print(\"Training everything\")\n",
    "    # gpflow.utilities.set_trainable(marginal_model.kernel, True)\n",
    "    # gpflow.utilities.set_trainable(marginal_model.likelihood, True)\n",
    "    # gpflow.utilities.set_trainable(marginal_model.X_data_mean , True)\n",
    "    # gpflow.utilities.set_trainable(marginal_model.X_data_var, True)\n",
    "    # gpflow.utilities.set_trainable(marginal_model.inducing_variable, True)\n",
    "    # opt = gpflow.optimizers.Scipy()\n",
    "    # opt_logs = opt.minimize(\n",
    "    #     marginal_model.training_loss,\n",
    "    #     marginal_model.trainable_variables,\n",
    "    #     options=dict(maxiter=10000),\n",
    "    # )\n",
    "    # causal_marg_loss = marginal_model.elbo()\n",
    "\n",
    "    # Define the prior\n",
    "    sq_exp = gpflow.kernels.Matern52(\n",
    "        lengthscales=[0.01]\n",
    "    )\n",
    "    linear_kernel = gpflow.kernels.Linear(variance=1)\n",
    "    sq_exp.variance.assign(1)\n",
    "    kernel = gpflow.kernels.Sum([sq_exp, linear_kernel])\n",
    "    # Define moedl\n",
    "    reg_gp_model = gpflow.models.GPR(data=(x, y), kernel=kernel, mean_function=None)\n",
    "    reg_gp_model.likelihood.variance = Parameter(\n",
    "        0.1, transform=positive(lower=1e-6)\n",
    "    )\n",
    "    # Train everything\n",
    "    tf.print(\"Training everything\")\n",
    "    gpflow.utilities.set_trainable(reg_gp_model.kernel, True)\n",
    "    gpflow.utilities.set_trainable(reg_gp_model.likelihood, True)\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    opt_logs = opt.minimize(\n",
    "        reg_gp_model.training_loss,\n",
    "        reg_gp_model.trainable_variables,\n",
    "        options=dict(maxiter=10000),\n",
    "    )\n",
    "    causal_cond_loss = reg_gp_model.log_marginal_likelihood().numpy()\n",
    "\n",
    "    full_causal = causal_cond_loss \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   # Define kernel\n",
    "#     sq_exp = gpflow.kernels.SquaredExponential(lengthscales=[0.01])\n",
    "#     sq_exp.variance.assign(1)\n",
    "#     linear_kernel = gpflow.kernels.Linear(variance=1)\n",
    "#     kernel = gpflow.kernels.Sum([sq_exp, linear_kernel])\n",
    "#     # Initialise approx posteroir and prior\n",
    "#     X_mean_init = 0.1 * tf.cast(y, default_float())\n",
    "#     X_var_init = tf.cast(\n",
    "#         np.random.uniform(0, 0.1, (y.shape[0], 1)), default_float()\n",
    "#     )\n",
    "#     x_prior_var = tf.ones((y.shape[0], 1), dtype=default_float())\n",
    "#     inducing_variable = gpflow.inducing_variables.InducingPoints(\n",
    "#         np.random.randn(500, 1),\n",
    "#     )\n",
    "\n",
    "#     # Define marginal model\n",
    "#     marginal_model = BayesianGPLVM(\n",
    "#         data=y,\n",
    "#         kernel=kernel,\n",
    "#         X_data_mean=X_mean_init,\n",
    "#         X_data_var=X_var_init,\n",
    "#         X_prior_var=x_prior_var,\n",
    "#         jitter=1e-4,\n",
    "#         inducing_variable=inducing_variable\n",
    "#     )\n",
    "#     marginal_model.likelihood.variance = Parameter(\n",
    "#         0.1, transform=positive(1e-6)\n",
    "#     )\n",
    "#     # Train everything\n",
    "#     tf.print(\"Training everything\")\n",
    "#     gpflow.utilities.set_trainable(marginal_model.kernel, True)\n",
    "#     gpflow.utilities.set_trainable(marginal_model.likelihood, True)\n",
    "#     gpflow.utilities.set_trainable(marginal_model.X_data_mean , True)\n",
    "#     gpflow.utilities.set_trainable(marginal_model.X_data_var, True)\n",
    "#     gpflow.utilities.set_trainable(marginal_model.inducing_variable, True)\n",
    "#     opt = gpflow.optimizers.Scipy()\n",
    "#     opt_logs = opt.minimize(\n",
    "#         marginal_model.training_loss,\n",
    "#         marginal_model.trainable_variables,\n",
    "#         options=dict(maxiter=10000),\n",
    "#     )\n",
    "#     anti_causal_marg_loss = marginal_model.elbo()\n",
    "\n",
    "    sq_exp = gpflow.kernels.Matern52(\n",
    "        lengthscales=[0.01]\n",
    "    )\n",
    "    linear_kernel = gpflow.kernels.Linear(variance=1)\n",
    "    sq_exp.variance.assign(1)\n",
    "    kernel = gpflow.kernels.Sum([sq_exp, linear_kernel])\n",
    "    # Define moedl\n",
    "    reg_gp_model = gpflow.models.GPR(data=(y, x), kernel=kernel, mean_function=None)\n",
    "    reg_gp_model.likelihood.variance = Parameter(\n",
    "        0.1, transform=positive(lower=1e-6)\n",
    "    )\n",
    "    tf.print(\"Training everything\")\n",
    "    gpflow.utilities.set_trainable(reg_gp_model.kernel, True)\n",
    "    gpflow.utilities.set_trainable(reg_gp_model.likelihood, True)\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    opt_logs = opt.minimize(\n",
    "        reg_gp_model.training_loss,\n",
    "        reg_gp_model.trainable_variables,\n",
    "        options=dict(maxiter=10000),\n",
    "    )\n",
    "    anti_causal_cond_loss = reg_gp_model.log_marginal_likelihood().numpy() \n",
    "\n",
    "    full_anti_causal = anti_causal_cond_loss \n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Run: {idx}, {full_causal}, {full_anti_causal}\")\n",
    "    # print(causal_marg_loss.numpy(), causal_cond_loss, anti_causal_marg_loss.numpy(), anti_causal_cond_loss )\n",
    "\n",
    "    if full_causal > full_anti_causal:\n",
    "        correct += 1\n",
    "\n",
    "print(correct / len(cause))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8d95aa7f610185758b0e4e87261a268c67bfd80b13e0d0c16090894e4422c34"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('gp-causal-3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
