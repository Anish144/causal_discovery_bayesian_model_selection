{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:28:57.353069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 10:28:57.353084: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import gpflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflow import covariances, kernels, likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gpflow import covariances, kernels, likelihoods\n",
    "from gpflow.base import Parameter, _cast_to_dtype\n",
    "from gpflow.config import default_float, default_jitter\n",
    "from gpflow.expectations import expectation\n",
    "from gpflow.inducing_variables import InducingPoints\n",
    "from gpflow.kernels import Kernel\n",
    "from gpflow.mean_functions import MeanFunction, Zero\n",
    "from gpflow.probability_distributions import DiagonalGaussian\n",
    "from gpflow.utilities import positive, to_default_float\n",
    "from gpflow.utilities.ops import pca_reduce\n",
    "from gpflow.models.gpr import GPR\n",
    "from gpflow.models.model import GPModel, MeanAndVariance\n",
    "from gpflow.models.training_mixins import InputData, InternalDataTrainingLossMixin, OutputData\n",
    "from gpflow.models.util import data_input_to_tensor, inducingpoint_wrapper\n",
    "from gpflow.covariances.dispatch import Kuf, Kuu\n",
    "\n",
    "\n",
    "class LVMOGP_(GPR):\n",
    "    \"\"\"\n",
    "    Latent Variable model where there are both observed and unobserved data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: OutputData,\n",
    "        latent_dim: int,\n",
    "        X_data_mean: Optional[tf.Tensor] = None,\n",
    "        kernel: Optional[Kernel] = None,\n",
    "        mean_function: Optional[MeanFunction] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise LVMOGP object. This method only works with a Gaussian likelihood.\n",
    "        :param data: y data matrix, size N (number of points) x D (dimensions)\n",
    "        :param latent_dim: the number of latent dimensions (Q)\n",
    "        :param X_data_mean: latent positions ([N, Q]), for the initialisation of the latent space.\n",
    "        :param kernel: kernel specification, by default Squared Exponential\n",
    "        :param mean_function: mean function, by default None.\n",
    "        \"\"\"\n",
    "        if X_data_mean is None:\n",
    "            X_data_mean = pca_reduce(data, latent_dim)\n",
    "\n",
    "        num_latent_gps = X_data_mean.shape[1]\n",
    "        if num_latent_gps != latent_dim:\n",
    "            msg = \"Passed in number of latent {0} does not match initial X {1}.\"\n",
    "            raise ValueError(msg.format(latent_dim, num_latent_gps))\n",
    "\n",
    "        if mean_function is None:\n",
    "            mean_function = Zero()\n",
    "\n",
    "        if kernel is None:\n",
    "            kernel = kernels.SquaredExponential(lengthscales=tf.ones((latent_dim,)))\n",
    "\n",
    "        if data.shape[1] < num_latent_gps:\n",
    "            raise ValueError(\"More latent dimensions than observed.\")\n",
    "\n",
    "        gpr_data = (Parameter(X_data_mean), data_input_to_tensor(data))\n",
    "        super().__init__(gpr_data, kernel, mean_function=mean_function)\n",
    "\n",
    "\n",
    "class LVMOGP(GPModel, InternalDataTrainingLossMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: OutputData,\n",
    "        X_data: tf.Tensor,\n",
    "        X_data_fn: tf.Tensor,\n",
    "        H_data_mean: tf.Tensor,\n",
    "        H_data_var: tf.Tensor,\n",
    "        kernel: Kernel,\n",
    "        num_inducing_variables: Optional[int] = None,\n",
    "        inducing_variable=None,\n",
    "        H_prior_mean=None,\n",
    "        H_prior_var=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise Bayesian LVMOGP object. This method only works with a Gaussian likelihood.\n",
    "        :param data: data matrix, size N (number of points) x D (dimensions)\n",
    "        :param X_data: initial latent positions, size N (number of points) x Q (latent dimensions).\n",
    "        :param X_data_var: variance of latent positions ([N, Q]), for the initialisation of the latent space.\n",
    "        :param kernel: kernel specification, by default Squared Exponential\n",
    "        :param num_inducing_variables: number of inducing points, M\n",
    "        :param inducing_variable: matrix of inducing points, size M (inducing points) x Q (latent dimensions). By default\n",
    "            random permutation of X_data_mean.\n",
    "        :param H_prior_mean: prior mean used in KL term of bound. By default 0. Same size as X_data_mean.\n",
    "        :param H_prior_var: prior variance used in KL term of bound. By default 1.\n",
    "        \"\"\"\n",
    "        num_data, num_latent_gps = X_data.shape\n",
    "        num_fns, num_latent_dims = H_data_mean.shape\n",
    "        super().__init__(kernel, likelihoods.Gaussian(), num_latent_gps=num_latent_gps)\n",
    "        self.data = data_input_to_tensor(data)\n",
    "\n",
    "        self.X_data = Parameter(X_data, trainable=False)\n",
    "        self.X_data_fn = Parameter(X_data_fn, trainable=False)\n",
    "        self.H_data_mean = Parameter(H_data_mean)\n",
    "        self.H_data_var = Parameter(H_data_var, transform=positive())\n",
    "\n",
    "        self.num_fns = num_fns\n",
    "        self.num_latent_dims = num_latent_dims\n",
    "        self.num_data = num_data\n",
    "        self.output_dim = self.data.shape[-1]\n",
    "\n",
    "        assert X_data.shape[0] == self.data.shape[0], \"X mean and Y must be same size.\"\n",
    "        assert H_data_mean.shape[0] == H_data_var.shape[0], \"H mean and var should be the same length\"\n",
    "\n",
    "        if (inducing_variable is None) == (num_inducing_variables is None):\n",
    "            raise ValueError(\n",
    "                \"BayesianGPLVM needs exactly one of `inducing_variable` and `num_inducing_variables`\"\n",
    "            )\n",
    "\n",
    "        if inducing_variable is None:\n",
    "            # By default we initialize by subset of initial latent points\n",
    "            # Note that tf.random.shuffle returns a copy, it does not shuffle in-\n",
    "            X_mean_tilde, X_var_tilde = self.fill_Hs()\n",
    "            Z = tf.random.shuffle(X_mean_tilde)[:num_inducing_variables]\n",
    "            inducing_variable = InducingPoints(Z)\n",
    "\n",
    "        self.inducing_variable = inducingpoint_wrapper(inducing_variable)\n",
    "\n",
    "        assert X_data.shape[1] == self.num_latent_gps\n",
    "\n",
    "        # deal with parameters for the prior mean variance of X\n",
    "        if H_prior_mean is None:\n",
    "            H_prior_mean = tf.zeros((self.num_fns, self.num_latent_dims), dtype=default_float())\n",
    "        if H_prior_var is None:\n",
    "            H_prior_var = tf.ones((self.num_fns, self.num_latent_dims))\n",
    "\n",
    "        self.H_prior_mean = tf.convert_to_tensor(np.atleast_1d(H_prior_mean), dtype=default_float())\n",
    "        self.H_prior_var = tf.convert_to_tensor(np.atleast_1d(H_prior_var), dtype=default_float())\n",
    "\n",
    "        assert self.H_prior_mean.shape[0] == self.num_fns\n",
    "        assert self.H_prior_mean.shape[1] == self.num_latent_dims\n",
    "        assert self.H_prior_var.shape[0] == self.num_fns\n",
    "        assert self.H_prior_var.shape[1] == self.num_latent_dims\n",
    "\n",
    "    def fill_Hs(self):\n",
    "        \"\"\"append Hs to Xs by function number\"\"\"\n",
    "\n",
    "        H_mean_vect = tf.reshape(tf.gather(_cast_to_dtype(self.H_data_mean, dtype=default_float()),\n",
    "                                      _cast_to_dtype(self.X_data_fn, dtype=tf.int64)),\n",
    "                                   [self.num_data, self.num_latent_dims])\n",
    "        H_var_vect = tf.reshape(tf.gather(_cast_to_dtype(self.H_data_var, dtype=default_float()),\n",
    "                                           _cast_to_dtype(self.X_data_fn, dtype=tf.int64)),\n",
    "                                   [self.num_data, self.num_latent_dims])\n",
    "\n",
    "        return tf.concat([self.X_data, H_mean_vect], axis=1),  \\\n",
    "               tf.concat([tf.zeros(self.X_data.shape, dtype=default_float()), H_var_vect], axis=1)\n",
    "\n",
    "    def maximum_log_likelihood_objective(self) -> tf.Tensor:\n",
    "        return self.elbo()\n",
    "\n",
    "    def elbo(self) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Construct a tensorflow function to compute the bound on the marginal\n",
    "        likelihood.\n",
    "        \"\"\"\n",
    "        Y_data = self.data\n",
    "        mu, var = self.fill_Hs()\n",
    "        pH = DiagonalGaussian(mu, var)\n",
    "        # pX = DiagonalGaussian(self.X_data, self.X_data_var)\n",
    "\n",
    "        num_inducing = self.inducing_variable.num_inducing\n",
    "        psi0 = tf.reduce_sum(expectation(pH, self.kernel))\n",
    "        psi1 = expectation(pH, (self.kernel, self.inducing_variable))\n",
    "        psi2 = tf.reduce_sum(\n",
    "            expectation(\n",
    "                pH, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "        cov_uu = covariances.Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n",
    "        L = tf.linalg.cholesky(cov_uu)\n",
    "        sigma2 = self.likelihood.variance\n",
    "        sigma = tf.sqrt(sigma2)\n",
    "\n",
    "        # Compute intermediate matrices\n",
    "        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma\n",
    "        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)\n",
    "        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2\n",
    "        B = AAT + tf.eye(num_inducing, dtype=default_float())\n",
    "        # tf.print(B)\n",
    "        LB = tf.linalg.cholesky(B)\n",
    "        log_det_B = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))\n",
    "        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma\n",
    "\n",
    "        # KL[q(x) || p(x)]\n",
    "        dH_data_var = (\n",
    "            self.H_data_var\n",
    "            if self.H_data_var.shape.ndims == 2\n",
    "            else tf.linalg.diag_part(self.H_data_var)\n",
    "        )\n",
    "        NQ = to_default_float(tf.size(self.H_data_mean))\n",
    "        D = to_default_float(tf.shape(Y_data)[1])\n",
    "        KL = -0.5 * tf.reduce_sum(tf.math.log(dH_data_var))\n",
    "        KL += 0.5 * tf.reduce_sum(tf.math.log(self.H_prior_var))\n",
    "        KL -= 0.5 * NQ\n",
    "        KL += 0.5 * tf.reduce_sum(\n",
    "            (tf.square(self.H_data_mean - self.H_prior_mean) + dH_data_var) / self.H_prior_var\n",
    "        )\n",
    "\n",
    "        # compute log marginal bound\n",
    "        ND = to_default_float(tf.size(Y_data))\n",
    "        bound = -0.5 * ND * tf.math.log(2 * np.pi * sigma2)\n",
    "        bound += -0.5 * D * log_det_B\n",
    "        bound += -0.5 * tf.reduce_sum(tf.square(Y_data)) / sigma2\n",
    "        bound += 0.5 * tf.reduce_sum(tf.square(c))\n",
    "        bound += -0.5 * D * (tf.reduce_sum(psi0) / sigma2 - tf.reduce_sum(tf.linalg.diag_part(AAT)))\n",
    "        bound -= KL\n",
    "        # tf.print(bound)\n",
    "        return bound\n",
    "\n",
    "    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the latent function at some new points\n",
    "        Xnew. For a derivation of the terms in here, see the associated SGPR\n",
    "        notebook.\n",
    "        \"\"\"\n",
    "        Y_data = self.data\n",
    "        X_data = self.X_data\n",
    "        X_mean_tilde, X_var_tilde = self.fill_Hs()\n",
    "        num_inducing = self.inducing_variable.num_inducing\n",
    "        err = Y_data - self.mean_function(X_data)\n",
    "        kuf = Kuf(self.inducing_variable, self.kernel, X_mean_tilde)\n",
    "        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())\n",
    "        Kus = Kuf(self.inducing_variable, self.kernel, Xnew)\n",
    "        sigma = tf.sqrt(self.likelihood.variance)\n",
    "        L = tf.linalg.cholesky(kuu)\n",
    "        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma\n",
    "        B = tf.linalg.matmul(A, A, transpose_b=True) + tf.eye(num_inducing, dtype=default_float())\n",
    "        LB = tf.linalg.cholesky(B)\n",
    "        Aerr = tf.linalg.matmul(A, err)\n",
    "        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma\n",
    "        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)\n",
    "        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)\n",
    "        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)\n",
    "        if full_cov:\n",
    "            var = (\n",
    "                self.kernel(Xnew)\n",
    "                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)\n",
    "                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)\n",
    "            )\n",
    "            var = tf.tile(var[None, ...], [self.num_latent_gps, 1, 1])  # [P, N, N]\n",
    "        else:\n",
    "            var = (\n",
    "                self.kernel(Xnew, full_cov=False)\n",
    "                + tf.reduce_sum(tf.square(tmp2), 0)\n",
    "                - tf.reduce_sum(tf.square(tmp1), 0)\n",
    "            )\n",
    "            var = tf.tile(var[:, None], [1, 1]) # self.num_latent_gps\n",
    "        # tf.print(self.kernel(Xnew, full_cov=False), summarize=-1)\n",
    "        # tf.print(A, summarize=-1)\n",
    "        # tf.print(B, summarize=-1)\n",
    "        return mean + self.mean_function(Xnew), var\n",
    "\n",
    "\n",
    "    def predict_log_density(self, data: OutputData) -> tf.Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'data', 'in_data', 'X_data_mean', 'X_data_var', and 'kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/vol/bitbucket/ad6013/Research/gp-causal/notebooks/gplvm.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bphlogiston.doc.ic.ac.uk/vol/bitbucket/ad6013/Research/gp-causal/notebooks/gplvm.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m PartObsBayesianGPLVM()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 5 required positional arguments: 'data', 'in_data', 'X_data_mean', 'X_data_var', and 'kernel'"
     ]
    }
   ],
   "source": [
    "PartObsBayesianGPLVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.utilities.freeze"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29dfc3a58ca91141e6b31b77b731cbdeae0adeabff34291556b287f724fc891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('gp-causal-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
