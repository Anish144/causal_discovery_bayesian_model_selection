{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Synthetic and Real results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will calculate the final results for the synthetic and real data.\n",
    "\n",
    "The datasets are:\n",
    "1. CE-Cha\n",
    "2. CE-Multi\n",
    "3. CE-Net\n",
    "4. CE-Gauss\n",
    "5. CE-Tueb\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKDIR should point to the top level folder!\n",
    "# Otherwise the CE-Tueb results may not load\n",
    "WORKDIR = \"./causal_discovery_bayesian_model_selection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(WORKDIR)\n",
    "import dill\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gplvm_causal_discovery.data import get_data\n",
    "from tqdm import trange\n",
    "from gplvm_causal_discovery.utils import BEST_SCORES\n",
    "from gplvm_causal_discovery.utils import return_all_scores, return_best_causal_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pairs(data_name):\n",
    "    synth_data_names = [\"cha\", \"multi\", \"net\", \"gauss\"]\n",
    "    if data_name in synth_data_names:\n",
    "        data_get = getattr(get_data, f\"get_{data_name}_pairs_dataset\")\n",
    "        x, y, weight, target = data_get(data_path=f\"{WORKDIR}/gplvm_causal_discovery/data/{data_name}_pairs/files\")\n",
    "    elif data_name == \"tueb\":\n",
    "        data_get = getattr(get_data, f\"get_tubingen_pairs_dataset\")\n",
    "        x, y, weight, target = data_get(data_path=f\"{WORKDIR}/gplvm_causal_discovery/data/pairs/files\")\n",
    "    else:\n",
    "        full_data_name = synth_data_names.append(\"tueb\")\n",
    "        raise ValueError(f\"data_name variable must be in {full_data_name}\")\n",
    "    return x, y, weight, target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File types for the results\n",
    "# The files may have different formats of results saved\n",
    "# For some datasets, there are multiple result files with different names\n",
    "# these will be returned as a list and the best results chosen \n",
    "# Indicator next to file will indicate what kind of format the results are\n",
    "# stored in\n",
    "\n",
    "def cha_result_files():\n",
    "    file_1 = [\n",
    "        f\"fullscore-cha_pairs-gplvm-reinit20-numind200_start:{i}_end:{i+150}.p\"\n",
    "        for i in np.linspace(0, 150, 2, dtype=int)\n",
    "    ]\n",
    "    return [(file_1, 0)]\n",
    "\n",
    "\n",
    "def multi_result_files():\n",
    "    files_1 = [\n",
    "        f\"fullscore-multi_pairs-gplvm-reinit20-numind200_start:{i}_end:{i+20}.p\"\n",
    "        for i in np.linspace(0, 280, 15, dtype=int)\n",
    "    ]\n",
    "    files_2 = [\n",
    "        f\"fullscore-multi_pairs-gplvm_adam-reinit2-numind200_start:{i}_end:{i+20}.p\"\n",
    "        for i in np.linspace(0, 280, 15, dtype=int)\n",
    "    ]\n",
    "    return [(files_2, 0), (files_1, 0)]\n",
    "\n",
    "\n",
    "def net_result_files():\n",
    "    files = [\n",
    "        f\"fullscore-net_pairs-gplvm-reinit20-numind200_start:{i}_end:{i+20}.p\"\n",
    "        for i in np.linspace(0, 280, 15, dtype=int)\n",
    "    ]\n",
    "    return [(files, 0)]\n",
    "\n",
    "\n",
    "def gauss_result_files():\n",
    "    files_1 = [\n",
    "        f\"fullscore-gauss_pairs-gplvm-reinit20-numind200_start:{i}_end:{i+20}.p\"\n",
    "        for i in np.linspace(0, 280, 15, dtype=int)\n",
    "    ]\n",
    "    files_2 = [\n",
    "        f\"fullscore-gauss_pairs-gplvm_adam-reinit2-numind200_start:{i}_end:{i+20}.p\"\n",
    "        for i in np.linspace(0, 280, 15, dtype=int)\n",
    "    ]\n",
    "    return [ (files_1, 0), (files_2, 0)]\n",
    "\n",
    "\n",
    "def tueb_result_files():\n",
    "    files_1 = [\n",
    "        f\"fullscore-cep-gplvmgeneralised-reinit10-numind200_start:{i}_end:{i+5}.p\"\n",
    "        for i in np.linspace(0, 100, 21, dtype=int)\n",
    "    ]    \n",
    "    return [(files_1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to put the results in the right format\n",
    "def extract_file(file_name):\n",
    "    with open(f\"{WORKDIR}/gplvm_causal_discovery/results/{file_name}\", \"rb\") as f:\n",
    "        result = dill.load(f)\n",
    "    return result\n",
    "\n",
    "\n",
    "def find_best_scores_tuples(all_files: list):\n",
    "    all_best_scores = {}\n",
    "    # Need a file counter so that the keys are the correct number\n",
    "    file_counter = 0 \n",
    "    for file in all_files:\n",
    "        result = extract_file(file)\n",
    "        for idx, scores in enumerate(result[\"scores\"]):\n",
    "            best_scores = BEST_SCORES(\n",
    "                scores[0][0], scores[0][1], scores[1][0], scores[1][1]\n",
    "            )\n",
    "            all_best_scores[idx + file_counter] = best_scores\n",
    "        # Need to find the index where the file ends\n",
    "        ending_idx = file[:-2].split(':')[-1]\n",
    "        file_counter = int(ending_idx)\n",
    "    return all_best_scores\n",
    "            \n",
    "\n",
    "def find_best_scores_from_dict(all_files: list):\n",
    "    all_results = {}\n",
    "    for file in all_files:\n",
    "        result = extract_file(file)\n",
    "        all_results.update(result[\"final_scores\"])\n",
    "\n",
    "    all_x, all_y_x, all_y, all_x_y = return_all_scores(all_results)\n",
    "    best_scores = return_best_causal_scores(all_x, all_y_x, all_y, all_x_y)\n",
    "    return best_scores\n",
    "\n",
    "\n",
    "def choose_best_scores(score_dict: dict):\n",
    "    \"\"\"Given multiple best score dicts, this will choose the best scores among\n",
    "    them.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): Values are dicts with value of form BEST_SCORES \n",
    "        which is a names tuple with arguements 'best_loss_x best_loss_y_x \n",
    "        best_loss_y best_loss_x_y'.\n",
    "    \"\"\"\n",
    "    if len(score_dict.keys()) == 1:\n",
    "        return score_dict[0]\n",
    "    else:\n",
    "        num_datasets = len(score_dict[0].keys())\n",
    "        final_best_score = {}\n",
    "        # For each run, find the best score\n",
    "        all_run_idxs = score_dict[0].keys()\n",
    "        for run_idx in list(all_run_idxs):\n",
    "            all_dicts_this_run = [score_dict[i][run_idx] for i in list(score_dict.keys())]\n",
    "            x_scores = [d_loop.best_loss_x for d_loop in all_dicts_this_run]\n",
    "            y_x_scores = [d_loop.best_loss_y_x for d_loop in all_dicts_this_run]\n",
    "            y_scores = [d_loop.best_loss_y for d_loop in all_dicts_this_run]\n",
    "            x_y_scores = [d_loop.best_loss_x_y for d_loop in all_dicts_this_run]\n",
    "            best_scores_this_run = BEST_SCORES(\n",
    "                min(x_scores), min(y_x_scores), min(y_scores), min(x_y_scores)\n",
    "            )\n",
    "            final_best_score[run_idx] = best_scores_this_run\n",
    "        return final_best_score\n",
    "\n",
    "\n",
    "def get_final_scores_from_best_scores(best_scores):\n",
    "    total_runs = len(list(best_scores.keys()))\n",
    "    y_scores = {}\n",
    "    y_scores_array = np.zeros(total_runs)\n",
    "    for idx, run_idx in enumerate(list(best_scores.keys())):\n",
    "        current_run = best_scores[run_idx]\n",
    "        causal_score = current_run.best_loss_x + current_run.best_loss_y_x\n",
    "        anticausal_score = current_run.best_loss_y + current_run.best_loss_x_y\n",
    "        y_scores[run_idx] = - causal_score + anticausal_score\n",
    "        y_scores_array[idx] = - causal_score + anticausal_score \n",
    "    return y_scores, y_scores_array\n",
    "\n",
    "\n",
    "def balance_for_auc(target, pred_scores):\n",
    "    # Targets are {-1, 1}, need to make sure it sums to zero\n",
    "    balance = int(np.sum(target))\n",
    "    if balance != 0:\n",
    "        # There are more negative examples\n",
    "        if balance < 0:\n",
    "            switch_cand_idx = np.nonzero(target < 0)[0]\n",
    "        # There are more positive examples here\n",
    "        else:\n",
    "            switch_cand_idx = np.nonzero(target > 0)[0]\n",
    "        # get \"balance\" number of indices\n",
    "        switch_idx = np.random.choice(\n",
    "            switch_cand_idx, size=int(np.abs(balance) // 2), replace=False\n",
    "        )\n",
    "        final_target = target.copy()\n",
    "        final_target[switch_idx] *= -1\n",
    "        final_pred_scores = pred_scores.copy()\n",
    "        final_pred_scores[switch_idx] *= -1\n",
    "    else:\n",
    "        final_target = target.copy()\n",
    "        final_pred_scores = pred_scores.copy()\n",
    "    if (balance % 2) == 0:\n",
    "        assert np.sum(final_target) == 0\n",
    "    else:\n",
    "        assert np.abs(np.sum(final_target)) == 1\n",
    "    return final_target, final_pred_scores\n",
    "    \n",
    "\n",
    "def calculate_auc(target, pred_scores, num_shuffles=1000):\n",
    "    # Need to make sure that the classes are evenly balanced\n",
    "    auc_all = []\n",
    "    for i in trange(num_shuffles):\n",
    "        total_runs = len(target)\n",
    "        flip_idx = np.random.choice(np.arange(total_runs), total_runs // 2, replace=False)\n",
    "        for i in range(total_runs):\n",
    "            if i in flip_idx:\n",
    "                target[i] *= -1\n",
    "                pred_scores[i] *= -1\n",
    "        final_target, final_pred_scores = balance_for_auc(target, pred_scores)\n",
    "        roc_auc = roc_auc_score(final_target, final_pred_scores)\n",
    "        auc_all.append(roc_auc)\n",
    "    return np.mean(auc_all)\n",
    "\n",
    "\n",
    "def return_auc_results(data_name):\n",
    "    # Check the data name is correct\n",
    "    all_data_names = [\"cha\", \"multi\", \"net\", \"gauss\", \"tueb\"]\n",
    "    if data_name not in all_data_names:\n",
    "        raise ValueError(f\"data_name is not correct. Must be one of {all_data_names}\")\n",
    "    # Get the result files\n",
    "    all_result_files = eval(f\"{data_name}_result_files\")()\n",
    "    # processing of result file will be different for different data\n",
    "    # if the number of files is more than 1, we need to select the best one\n",
    "    best_score_dict = {}\n",
    "    for idx, result_files in enumerate(all_result_files):\n",
    "        actual_files, indicator = result_files\n",
    "        if indicator == 0:\n",
    "            best_scores = find_best_scores_tuples(all_files=actual_files)\n",
    "        else:\n",
    "            best_scores = find_best_scores_from_dict(all_files=actual_files)\n",
    "        best_score_dict[idx] = best_scores\n",
    "    final_best_score = choose_best_scores(best_score_dict)\n",
    "    # Find the final score by adding the causla and anticausal scores\n",
    "    _, final_scores = get_final_scores_from_best_scores(best_scores=final_best_score)\n",
    "    if data_name != \"tueb\":\n",
    "        _, _, _, target =  return_pairs(data_name=data_name)\n",
    "    else:\n",
    "        target = np.ones(99)\n",
    "    auc = calculate_auc(target=target, pred_scores=final_scores)\n",
    "    return auc, final_scores, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi is missing a run (run number 199 due to NaNs)\n",
    "for dn in [\"cha\", \"multi\", \"net\", \"gauss\", \"tueb\"]:\n",
    "    auc, score, target = return_auc_results(data_name=dn)\n",
    "    print(f\"{dn}: {auc}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8d95aa7f610185758b0e4e87261a268c67bfd80b13e0d0c16090894e4422c34"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('gp-causal-3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
