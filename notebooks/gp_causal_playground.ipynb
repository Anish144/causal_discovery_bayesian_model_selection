{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beginning-crash",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greek-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/anishdhir/Documents/gp-causal/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twelve-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pairs.generate_pairs import TubingenPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contained-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load cause-effect pairs: 100%|██████████| 100/100 [00:00<00:00, 167.17it/s]\n"
     ]
    }
   ],
   "source": [
    "data_gen = TubingenPairs(path='/Users/anishdhir/Documents/gp-causal/data/pairs/files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integrated-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, weight = [], [], []\n",
    "for i in data_gen.pairs_generator():\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "    weight.append(i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b2d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "widespread-correction",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "\n",
    "Check for linearity etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decreased-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-football",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "savepath = \"/Users/anish.dhir/Documents/Research/gp_causal/tuebingen_plots\"\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    if x[i].shape[-1] == 1:\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.title(f\"Plot {i}\")\n",
    "        plt.savefig(f\"{savepath}/Tuebingen: {i}\")\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-surgeon",
   "metadata": {},
   "source": [
    "## Try and train a GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-running",
   "metadata": {},
   "source": [
    "The basic premise here is to use the properties of the marginal likelihood to try and compare the the complexities of cause|effect vs. effect|cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "353f8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "\n",
    "class LogKernel(gpytorch.kernels.Kernel):\n",
    "    is_stationary = True\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        diff = self.covar_dist(x1, x2, square_dist=True, **params)\n",
    "        diff.where(diff == 0, torch.as_tensor(1e-20))\n",
    "        kern = torch.log(diff + 1)\n",
    "        return kern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "specialized-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5)) + gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel())\n",
    "        # self.covar_module = gpytorch.kernels.ScaleKernel(LogKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "advanced-spring",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "def train(x, y, num_inducing):\n",
    "    if len(x) < num_inducing + 1:\n",
    "        inducing = x_train\n",
    "    else:\n",
    "        # kmeans = KMeans(n_clusters=num_inducing).fit(x_train)\n",
    "        # inducing = kmeans.cluster_centers_\n",
    "        inducing_idx = np.random.choice(x_train.size(0), replace=False)\n",
    "        inducing = np.take(x_train, inducing_idx, axis=0).float()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(inducing)\n",
    "    # Set initial lengthscale value\n",
    "    init_lengthscale = 1\n",
    "    init_scale = 1\n",
    "    for kern_idx in range(len(model.covar_module.kernels) - 1 ):\n",
    "        model.covar_module.kernels[kern_idx].base_kernel.lengthscale = init_lengthscale\n",
    "        model.covar_module.kernels[kern_idx].outputscale = init_scale\n",
    "    likelihood.noise = 2.\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},  # Includes GaussianLikelihood parameters\n",
    "    ], lr=0.01)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y.size(0))\n",
    "\n",
    "    training_iter = 10000\n",
    "    loss_list = []\n",
    "    t = trange(training_iter, desc=\"Running Model\", leave=True, position=0)\n",
    "    for i in t:\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = - mll(output, y[:, 0])\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "        if i > 1000:\n",
    "            # Need a convergence criteria\n",
    "            if np.abs(np.mean(loss_list[-10:]) - np.mean(loss_list[-50:-10])) < np.std(loss_list[-50:-10]):\n",
    "                break\n",
    "        if i % 25 == 0:\n",
    "            t.set_description(f\"Loss: {loss}\")\n",
    "            t.refresh()\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "extensive-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/p7hrkmz13tq8d8mzztxp30tc0000gn/T/ipykernel_40285/4075799972.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(len(x)), desc=\"Epochs\", leave=True, position=0):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9c357c6c2049548548a47a18cd2342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4191029071807861:  59%|█████▉    | 5936/10000 [01:52<01:17, 52.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lv/p7hrkmz13tq8d8mzztxp30tc0000gn/T/ipykernel_40285/4075799972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munif_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inducing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inducing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mloss_x_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inducing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inducing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# x <- y score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lv/p7hrkmz13tq8d8mzztxp30tc0000gn/T/ipykernel_40285/2270945479.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, y, num_inducing)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Calc loss and backprop gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/causal_data_aug-3.8.12/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/causal_data_aug-3.8.12/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "correct_idx = []\n",
    "wrong_idx = []\n",
    "num_inducing = 500\n",
    "\n",
    "for i in tqdm(range(len(x)), desc=\"Epochs\", leave=True, position=0):\n",
    "    print(f'\\n {i}')\n",
    "    # Ignore the high dim\n",
    "    if x[i].shape[-1] > 1:\n",
    "        continue\n",
    "    else:\n",
    "        # Get data points\n",
    "        x_train, y_train, weight_train = x[i], y[i], weight[i]\n",
    "    # Make sure data is standardised \n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    y_train = StandardScaler().fit_transform(y_train)\n",
    "    x_train, y_train = torch.from_numpy(x_train.astype(float)).float(), torch.from_numpy(y_train.astype(float)).float()\n",
    "    # x -> y score\n",
    "    # Draw uniform \n",
    "    unif_samples = torch.rand(\n",
    "        x_train.size(0), 1\n",
    "    )\n",
    "    loss_x = train(x=unif_samples, y=x_train, num_inducing=num_inducing)\n",
    "    loss_x_y = train(x=x_train, y=y_train, num_inducing=num_inducing)\n",
    "    # x <- y score\n",
    "    # Draw uniform \n",
    "    unif_samples = torch.rand(\n",
    "        y_train.size(0), 1\n",
    "    )\n",
    "    loss_y = train(x=unif_samples, y=y_train, num_inducing=num_inducing)\n",
    "    loss_y_x = train(x=y_train, y=x_train, num_inducing=num_inducing)\n",
    "    # Calculate losses\n",
    "    score_x_y = loss_x[-1] + loss_x_y[-1]\n",
    "    score_y_x = loss_y[-1] + loss_y_x[-1]\n",
    "    if score_x_y < score_y_x:\n",
    "        correct_idx.append(i)\n",
    "    else:\n",
    "        wrong_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_weight = [weight[i] for i in correct_idx]\n",
    "wrong_weight = [weight[i] for i in wrong_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(correct_weight) / (np.sum(correct_weight) + np.sum(wrong_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-screening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "crazy-springer",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "\"/usr/local/bin/fish\" shell not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lv/p7hrkmz13tq8d8mzztxp30tc0000gn/T/ipykernel_34807/2713477602.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install scikit-learn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/causal_data_aug-3.8.12/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/causal_data_aug-3.8.12/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pexpect-U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/causal_data_aug-3.8.12/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshell_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"{}\" shell not found'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshell_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: \"/usr/local/bin/fish\" shell not found"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c5354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
