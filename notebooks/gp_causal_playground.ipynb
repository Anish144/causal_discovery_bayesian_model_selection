{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beginning-crash",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greek-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/anish.dhir/Documents/Research/gp_causal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twelve-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pairs.generate_pairs import TubingenPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contained-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load cause-effect pairs: 100%|██████████| 108/108 [00:02<00:00, 50.82it/s]\n"
     ]
    }
   ],
   "source": [
    "data_gen = TubingenPairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integrated-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, weight = [], [], []\n",
    "for i in data_gen.pairs_generator():\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "    weight.append(i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-correction",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "\n",
    "Check for linearity etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-football",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "savepath = \"/Users/anish.dhir/Documents/Research/gp_causal/tuebingen_plots\"\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    if x[i].shape[-1] == 1:\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.title(f\"Plot {i}\")\n",
    "        plt.savefig(f\"{savepath}/Tuebingen: {i}\")\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-surgeon",
   "metadata": {},
   "source": [
    "## Try and train a GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-running",
   "metadata": {},
   "source": [
    "The basic premise here is to use the properties of the marginal likelihood to try and compare the the complexities of cause|effect vs. effect|cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specialized-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advanced-spring",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "def train(x, y, num_inducing):\n",
    "    if len(x) < num_inducing + 1:\n",
    "        inducing = x_train\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=num_inducing).fit(x_train)\n",
    "        inducing = kmeans.cluster_centers_\n",
    "        inducing = torch.from_numpy(inducing.astype(float)).float()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(inducing)\n",
    "    # Set initial lengthscale value\n",
    "    init_lengthscale = torch.std(x) / 10\n",
    "    model.covar_module.base_kernel.lengthscale = init_lengthscale\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "    ], lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    training_iter = 10000\n",
    "    loss_list = []\n",
    "    t = trange(training_iter, desc=\"Running Model\", leave=True, position=0)\n",
    "    for i in t:\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = - mll(output, y[:, 0])\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.step()\n",
    "        if i > 1000:\n",
    "            # Need a convergence criteria\n",
    "            if np.abs(np.mean(loss_list[-10:]) - np.mean(loss_list[-50:-10])) < np.std(loss_list[-50:-10]):\n",
    "                break\n",
    "        if i % 25 == 0:\n",
    "            t.set_description(f\"Loss: {loss}\")\n",
    "            t.refresh()\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extensive-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anish.dhir/.pyenv/versions/3.6.8/envs/gp_causal_3.6.8/lib/python3.6/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41be5035e786487bb96076325ededbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8480803370475769:  10%|█         | 1015/10000 [01:01<09:06, 16.43it/s]\n",
      "Loss: 0.8582813739776611:  10%|█         | 1001/10000 [00:54<08:09, 18.40it/s]\n",
      "Loss: 0.9538010954856873:  10%|█         | 1015/10000 [01:07<09:56, 15.06it/s]\n",
      "Loss: 0.9686558842658997:  10%|█         | 1025/10000 [01:23<12:07, 12.33it/s]\n",
      "Loss: 1.3418712615966797:  10%|█         | 1008/10000 [03:19<29:37,  5.06it/s]\n",
      "Loss: 1.2655020952224731:  10%|█         | 1001/10000 [01:11<10:41, 14.03it/s]\n",
      "Loss: 1.2926459312438965:  10%|█         | 1001/10000 [03:49<34:26,  4.36it/s]\n",
      "Loss: 1.297794222831726:  10%|█         | 1001/10000 [03:14<29:05,  5.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6d690e644fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# x -> y score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss_x_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inducing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inducing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# x <- y score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss_y_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inducing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inducing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-82ca52e9f76c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, y, num_inducing)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minducing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inducing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0minducing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minducing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minducing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/gp_causal_3.6.8/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 x_squared_norms=x_squared_norms, n_threads=self._n_threads)\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/gp_causal_3.6.8/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;31m# compute new pairwise distances between centers and closest other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# center of each center for next iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mcenter_half_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters_new\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         distance_next_center = np.partition(\n\u001b[1;32m    394\u001b[0m             np.asarray(center_half_distances), kth=1, axis=0)[1]\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/gp_causal_3.6.8/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/gp_causal_3.6.8/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# Ensure that distances between vectors and themselves are set to 0.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "correct_idx = []\n",
    "wrong_idx = []\n",
    "num_inducing = 2000\n",
    "\n",
    "for i in tqdm(range(len(x)), desc=\"Epochs\", leave=True, position=0):\n",
    "    # Ignore the high dim\n",
    "    if x[i].shape[-1] > 1:\n",
    "        continue\n",
    "    else:\n",
    "        # Get data points\n",
    "        x_train, y_train, weight_train = x[i], y[i], weight[i]\n",
    "    # Make sure data is standardised \n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    y_train = StandardScaler().fit_transform(y_train)\n",
    "    x_train, y_train = torch.from_numpy(x_train.astype(float)).float(), torch.from_numpy(y_train.astype(float)).float()\n",
    "    # x -> y score\n",
    "    loss_x_y = train(x=x_train, y=y_train, num_inducing=num_inducing)\n",
    "    # x <- y score\n",
    "    loss_y_x = train(x=y_train, y=x_train, num_inducing=num_inducing)\n",
    "    if loss_x_y[-1] < loss_y_x[-1]:\n",
    "        correct_idx.append(i)\n",
    "    else:\n",
    "        wrong_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_weight = [weight[i] for i in correct_idx]\n",
    "wrong_weight = [weight[i] for i in wrong_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(correct_weight) / (np.sum(correct_weight) + np.sum(wrong_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-screening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-springer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
